{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e03d2c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T16:58:59.348449Z",
     "start_time": "2021-05-02T16:58:55.943198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The code is used to train BC imitator, or pretrained GAIL imitator\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import tempfile\n",
    "import os.path as osp\n",
    "import gym\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "##from baselines.common.mpi_adam import MpiAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7609389e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:24:59.946218Z",
     "start_time": "2021-05-02T06:24:59.932029Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_expert_data_with_rewards(scenario_file, num_interactions=1000, file_name=\"expert_data\", act_ndim=19):\n",
    "    #from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "    import numpy as np\n",
    "    expert_observations = []\n",
    "    expert_actions = []\n",
    "    expert_rewards = []\n",
    "\n",
    "    gf_env_settings = {\n",
    "        \"stacked\": True,\n",
    "        \"rewards\": 'scoring',\n",
    "        \"representation\": 'extracted',\n",
    "        \"players\": [f\"agent:left_players=1\"],\n",
    "        \"action_set\": \"default\",#\"default\" \"v2\"\n",
    "    }\n",
    "\n",
    "    from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "    from scenic.simulators.gfootball.utilities.scenic_helper import buildScenario\n",
    "    scenario = buildScenario(scenario_file)\n",
    "    env = GFScenicEnv(initial_scenario=scenario, gf_env_settings=gf_env_settings, use_scenic_behavior_in_step=True, constraints_checking=True)\n",
    "    \n",
    "    obs = env.reset()\n",
    "    tr = 0\n",
    "    for i in tqdm(range(num_interactions)):\n",
    "        expert_observations.append(obs)\n",
    "\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        tr+=reward\n",
    "        # print(info)\n",
    "        action = info[\"action_taken\"]\n",
    "        expert_actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            expert_rewards.append(tr)\n",
    "            tr = 0\n",
    "\n",
    "    expert_observations = np.array(expert_observations)\n",
    "    #expert_observations = np.moveaxis(expert_observations, [3], [1])\n",
    "    acts = np.array(expert_actions)\n",
    "\n",
    "    acts_oh = np.zeros((acts.shape[0], act_ndim))\n",
    "    acts_oh[np.arange(acts.shape[0]), acts] = 1\n",
    "\n",
    "    expert_rewards = np.array(expert_rewards)\n",
    "    print(\"Expert observation shape: \", expert_observations.shape)\n",
    "    print(\"Expert actions shape: \", acts_oh.shape)\n",
    "    print(\"Num Episode: \", expert_rewards.shape[0])\n",
    "    print(\"Mean Reward: \", expert_rewards.mean())\n",
    "\n",
    "    #np.savez(expert_file_name, obs=mb_obs, acs=mb_act_oh, num_epi = num_episodes, mean_reward = np.sum(mb_rewards)/num_episodes)\n",
    "    np.savez_compressed(\n",
    "        file_name,\n",
    "        acs=acts_oh,\n",
    "        obs=expert_observations,\n",
    "        num_epi = expert_rewards.shape[0],\n",
    "        mean_reward = expert_rewards.mean(),\n",
    "        rewards = expert_rewards\n",
    "    )\n",
    "    return expert_observations, acts_oh, expert_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a661e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:25:01.753912Z",
     "start_time": "2021-05-02T06:25:01.735414Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_expert_successful_data(scenario_file, num_interactions=1000, file_name=\"expert_data\", act_ndim=19):\n",
    "    #from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "    import numpy as np\n",
    "    expert_observations = []\n",
    "    expert_actions = []\n",
    "    expert_rewards = []\n",
    "\n",
    "    gf_env_settings = {\n",
    "        \"stacked\": True,\n",
    "        \"rewards\": 'scoring',\n",
    "        \"representation\": 'extracted',\n",
    "        \"players\": [f\"agent:left_players=1\"],\n",
    "        \"action_set\": \"default\",#\"default\" \"v2\"\n",
    "    }\n",
    "\n",
    "    from scenic.simulators.gfootball.rl_interface import GFScenicEnv\n",
    "    from scenic.simulators.gfootball.utilities.scenic_helper import buildScenario\n",
    "    scenario = buildScenario(scenario_file)\n",
    "    env = GFScenicEnv(initial_scenario=scenario, gf_env_settings=gf_env_settings, use_scenic_behavior_in_step=True, constraints_checking=True)\n",
    "    \n",
    "    \n",
    "    tr = 0\n",
    "    \n",
    "    obs_buf, acts_buf, rew_buf  = [], [], []\n",
    "    policy_rews = []\n",
    "    \n",
    "    with tqdm(total=num_interactions) as pbar:\n",
    "        \n",
    "        while(len(expert_observations)<num_interactions):\n",
    "            \n",
    "            \n",
    "            if len(obs_buf)==0:\n",
    "                obs = env.reset()\n",
    "            \n",
    "            obs_buf.append(obs)\n",
    "            obs, reward, done, info = env.step(env.action_space.sample())\n",
    "            \n",
    "            #rew_buf.append(reward)\n",
    "\n",
    "            tr+=reward\n",
    "            # print(info)\n",
    "            action = info[\"action_taken\"]\n",
    "            acts_buf.append(action)\n",
    "\n",
    "            if done:\n",
    "                \n",
    "                #print(f\"New Epi: {len(obs_buf)} R: {tr}\")\n",
    "                \n",
    "                policy_rews.append(tr)\n",
    "                \n",
    "                if tr>0: \n",
    "                    expert_observations.extend(obs_buf)\n",
    "                    expert_actions.extend(acts_buf)\n",
    "                    expert_rewards.append(tr)\n",
    "                    \n",
    "                    if len(expert_observations)> num_interactions:\n",
    "                        pbar.update(num_interactions)\n",
    "                    else:\n",
    "                        pbar.update(len(obs_buf)) \n",
    "                        \n",
    "                    #print(\"Added new Api. Current Size: \", len(expert_observations))\n",
    "\n",
    "\n",
    "                obs_buf, acts_buf, rew_buf  = [], [], []\n",
    "                obs = env.reset()\n",
    "                tr = 0\n",
    "\n",
    "            \n",
    "    print(\"Collection Done\")        \n",
    "        \n",
    "\n",
    "    expert_observations = np.array(expert_observations)\n",
    "    #expert_observations = np.moveaxis(expert_observations, [3], [1])\n",
    "    acts = np.array(expert_actions)\n",
    "\n",
    "    acts_oh = np.zeros((acts.shape[0], act_ndim))\n",
    "    acts_oh[np.arange(acts.shape[0]), acts] = 1\n",
    "\n",
    "    expert_rewards = np.array(expert_rewards)\n",
    "    \n",
    "    print(\"Expert observation shape: \", expert_observations.shape)\n",
    "    print(\"Expert actions shape: \", acts_oh.shape)\n",
    "    print(\"Num Expert Episode: \", expert_rewards.shape[0])\n",
    "    print(\"Mean Expert Reward: \", expert_rewards.mean())\n",
    "    \n",
    "    print(\"Num Trajectories Collected: \", len(policy_rews))\n",
    "    print(\"Mean Policy Reward: \", np.mean(policy_rews))\n",
    "    \n",
    "\n",
    "    #np.savez(expert_file_name, obs=mb_obs, acs=mb_act_oh, num_epi = num_episodes, mean_reward = np.sum(mb_rewards)/num_episodes)\n",
    "    np.savez_compressed(\n",
    "        file_name,\n",
    "        acs=acts_oh,\n",
    "        obs=expert_observations,\n",
    "        num_epi = expert_rewards.shape[0],\n",
    "        mean_reward = expert_rewards.mean(),\n",
    "        rewards = expert_rewards,\n",
    "        policy_mean_reward = np.mean(policy_rews),\n",
    "        policy_total_trajectories = len(policy_rews)\n",
    "    )\n",
    "    return expert_observations, acts_oh, expert_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f371f49a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T07:48:31.722462Z",
     "start_time": "2021-05-02T07:04:59.973724Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment will ignore actions passed to step() and take action provided by Scenic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19973it [43:26,  7.66it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Done\n",
      "Expert observation shape:  (10024, 72, 96, 16)\n",
      "Expert actions shape:  (10024, 19)\n",
      "Num Expert Episode:  143\n",
      "Mean Expert Reward:  1.0\n",
      "Num Trajectories Collected:  813\n",
      "Mean Policy Reward:  0.17589175891758918\n"
     ]
    }
   ],
   "source": [
    "#from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "\n",
    "#scenario = \"/Users//codebase/scenic/training/gfrl/_scenarios/sc4rl/wb/fg_11v1_wb_rns_rand1.scenic\"\n",
    "scenario =\"/Users//codebase/scenic/training/gfrl/_scenarios/sc4rl/wb/ps_3v2_0_wb_0.scenic\"\n",
    "num_interactions = 10000\n",
    "expert_file_name = f\"../_data/sc4rl_ps_3v2_0_v0_rand0_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_successful_data(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cbef96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T16:59:42.943346Z",
     "start_time": "2021-05-02T16:59:40.422947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 1.0 10089 (10089, 19) policy episodes, mean rew:  143 0.6433566433566433\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#num_interactions=1000\n",
    "#expert_file_name = f\"../_data/sc4rl_3v1_v1_succ_{num_interactions}.npz\"\n",
    "\n",
    "expert_file_name = \"/Users//codebase/scenic/training/gfrl/_data/sc4rl_fg11v1_rns_rand1_succ_10000.npz\"\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape, \"policy episodes, mean rew: \", dataset.policy_total_trajectories, dataset.policy_mean_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "\n",
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=1000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "td = np.load(expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ad242",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(td.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=5000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfrl.base.run_my_ppo2 import create_single_scenic_environment\n",
    "\n",
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=10000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=20000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af70f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"../_scenarios/academy/wb/pass_n_shoot_wb.scenic\"\n",
    "\n",
    "num_interactions=50000\n",
    "expert_file_name = f\"../_data/pns_{num_interactions}.npz\"\n",
    "\n",
    "obs, act, rew = generate_expert_data_with_rewards(scenario_file=scenario, num_interactions=num_interactions, file_name=expert_file_name)\n",
    "\n",
    "from gfrl.common.mybase.cloning.dataset import GFDset\n",
    "dataset = GFDset(expert_file_name)\n",
    "print(dataset.num_epi, dataset.mean_reward, dataset.size, dataset.acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81023d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
